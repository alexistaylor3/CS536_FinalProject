{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76183219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080\n",
      "Memory Usage:\n",
      "Allocated: 0.1 GB\n",
      "Cached:    0.1 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lexi\\AppData\\Local\\Temp\\ipykernel_17712\\237759859.py:162: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
      "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "47000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Lexi/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "c:\\ProjectBreez\\gpu_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProjectBreez\\gpu_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 2.667, Ça coûte 14.868\n",
      "[1,  2000] loss: 1.981, Ça coûte 66.855\n",
      "[1,  3000] loss: 1.805, Ça coûte 65.805\n",
      "[1,  4000] loss: 1.693, Ça coûte 66.928\n",
      "[1,  5000] loss: 1.645, Ça coûte 65.330\n",
      "[2,  1000] loss: 1.526, Ça coûte 16.389\n",
      "[2,  2000] loss: 1.462, Ça coûte 67.450\n",
      "[2,  3000] loss: 1.469, Ça coûte 65.117\n",
      "[2,  4000] loss: 1.421, Ça coûte 63.557\n",
      "[2,  5000] loss: 1.404, Ça coûte 61.235\n",
      "[3,  1000] loss: 1.319, Ça coûte 16.556\n",
      "[3,  2000] loss: 1.278, Ça coûte 33.037\n",
      "[3,  3000] loss: 1.279, Ça coûte 31.847\n",
      "[3,  4000] loss: 1.244, Ça coûte 31.945\n",
      "[3,  5000] loss: 1.237, Ça coûte 31.703\n",
      "[4,  1000] loss: 1.174, Ça coûte 9.269\n",
      "[4,  2000] loss: 1.179, Ça coûte 31.748\n",
      "[4,  3000] loss: 1.156, Ça coûte 32.595\n",
      "[4,  4000] loss: 1.157, Ça coûte 32.664\n",
      "[4,  5000] loss: 1.143, Ça coûte 32.391\n",
      "[5,  1000] loss: 1.097, Ça coûte 9.165\n",
      "[5,  2000] loss: 1.079, Ça coûte 32.290\n",
      "[5,  3000] loss: 1.093, Ça coûte 32.053\n",
      "[5,  4000] loss: 1.088, Ça coûte 32.595\n",
      "[5,  5000] loss: 1.044, Ça coûte 32.409\n",
      "[6,  1000] loss: 1.032, Ça coûte 9.231\n",
      "[6,  2000] loss: 1.001, Ça coûte 30.984\n",
      "[6,  3000] loss: 1.018, Ça coûte 31.443\n",
      "[6,  4000] loss: 1.004, Ça coûte 31.421\n",
      "[6,  5000] loss: 1.019, Ça coûte 31.567\n",
      "[7,  1000] loss: 0.967, Ça coûte 9.123\n",
      "[7,  2000] loss: 0.969, Ça coûte 31.739\n",
      "[7,  3000] loss: 0.951, Ça coûte 32.405\n",
      "[7,  4000] loss: 0.971, Ça coûte 31.793\n",
      "[7,  5000] loss: 0.961, Ça coûte 31.957\n",
      "[8,  1000] loss: 0.909, Ça coûte 9.400\n",
      "[8,  2000] loss: 0.925, Ça coûte 31.668\n",
      "[8,  3000] loss: 0.907, Ça coûte 31.540\n",
      "[8,  4000] loss: 0.915, Ça coûte 32.026\n",
      "[8,  5000] loss: 0.915, Ça coûte 31.858\n",
      "[9,  1000] loss: 0.878, Ça coûte 9.137\n",
      "[9,  2000] loss: 0.875, Ça coûte 31.241\n",
      "[9,  3000] loss: 0.887, Ça coûte 32.122\n",
      "[9,  4000] loss: 0.871, Ça coûte 31.817\n",
      "[9,  5000] loss: 0.852, Ça coûte 32.270\n",
      "[10,  1000] loss: 0.821, Ça coûte 9.162\n",
      "[10,  2000] loss: 0.847, Ça coûte 31.614\n",
      "[10,  3000] loss: 0.828, Ça coûte 36.052\n",
      "[10,  4000] loss: 0.831, Ça coûte 37.241\n",
      "[10,  5000] loss: 0.839, Ça coûte 39.048\n",
      "[11,  1000] loss: 0.792, Ça coûte 13.913\n",
      "[11,  2000] loss: 0.823, Ça coûte 46.787\n",
      "[11,  3000] loss: 0.815, Ça coûte 43.830\n",
      "[11,  4000] loss: 0.799, Ça coûte 45.218\n",
      "[11,  5000] loss: 0.790, Ça coûte 46.762\n",
      "[12,  1000] loss: 0.763, Ça coûte 12.829\n",
      "[12,  2000] loss: 0.780, Ça coûte 47.014\n",
      "[12,  3000] loss: 0.758, Ça coûte 45.543\n",
      "[12,  4000] loss: 0.768, Ça coûte 46.634\n",
      "[12,  5000] loss: 0.783, Ça coûte 47.467\n",
      "[13,  1000] loss: 0.730, Ça coûte 13.859\n",
      "[13,  2000] loss: 0.731, Ça coûte 46.183\n",
      "[13,  3000] loss: 0.741, Ça coûte 43.042\n",
      "[13,  4000] loss: 0.750, Ça coûte 43.173\n",
      "[13,  5000] loss: 0.754, Ça coûte 45.653\n",
      "[14,  1000] loss: 0.716, Ça coûte 13.237\n",
      "[14,  2000] loss: 0.720, Ça coûte 45.212\n",
      "[14,  3000] loss: 0.722, Ça coûte 45.231\n",
      "[14,  4000] loss: 0.729, Ça coûte 45.967\n",
      "[14,  5000] loss: 0.689, Ça coûte 48.386\n",
      "[15,  1000] loss: 0.657, Ça coûte 13.212\n",
      "[15,  2000] loss: 0.675, Ça coûte 46.387\n",
      "[15,  3000] loss: 0.686, Ça coûte 46.311\n",
      "[15,  4000] loss: 0.689, Ça coûte 45.851\n",
      "[15,  5000] loss: 0.701, Ça coûte 45.980\n",
      "[16,  1000] loss: 0.643, Ça coûte 12.549\n",
      "[16,  2000] loss: 0.665, Ça coûte 45.866\n",
      "[16,  3000] loss: 0.662, Ça coûte 46.634\n",
      "[16,  4000] loss: 0.649, Ça coûte 46.765\n",
      "[16,  5000] loss: 0.663, Ça coûte 46.199\n",
      "[17,  1000] loss: 0.623, Ça coûte 12.748\n",
      "[17,  2000] loss: 0.640, Ça coûte 44.085\n",
      "[17,  3000] loss: 0.625, Ça coûte 44.970\n",
      "[17,  4000] loss: 0.639, Ça coûte 44.121\n",
      "[17,  5000] loss: 0.629, Ça coûte 45.481\n",
      "[18,  1000] loss: 0.600, Ça coûte 12.479\n",
      "[18,  2000] loss: 0.605, Ça coûte 45.081\n",
      "[18,  3000] loss: 0.607, Ça coûte 44.685\n",
      "[18,  4000] loss: 0.615, Ça coûte 43.837\n",
      "[18,  5000] loss: 0.613, Ça coûte 44.217\n",
      "[19,  1000] loss: 0.568, Ça coûte 12.524\n",
      "[19,  2000] loss: 0.573, Ça coûte 44.770\n",
      "[19,  3000] loss: 0.568, Ça coûte 44.039\n",
      "[19,  4000] loss: 0.597, Ça coûte 43.776\n",
      "[19,  5000] loss: 0.596, Ça coûte 44.731\n",
      "[20,  1000] loss: 0.539, Ça coûte 11.309\n",
      "[20,  2000] loss: 0.565, Ça coûte 44.053\n",
      "[20,  3000] loss: 0.566, Ça coûte 45.171\n",
      "[20,  4000] loss: 0.558, Ça coûte 44.538\n",
      "[20,  5000] loss: 0.555, Ça coûte 44.523\n",
      "[21,  1000] loss: 0.531, Ça coûte 11.366\n",
      "[21,  2000] loss: 0.530, Ça coûte 44.085\n",
      "[21,  3000] loss: 0.538, Ça coûte 45.143\n",
      "[21,  4000] loss: 0.536, Ça coûte 43.481\n",
      "[21,  5000] loss: 0.540, Ça coûte 43.729\n",
      "[22,  1000] loss: 0.504, Ça coûte 11.295\n",
      "[22,  2000] loss: 0.505, Ça coûte 44.470\n",
      "[22,  3000] loss: 0.505, Ça coûte 44.256\n",
      "[22,  4000] loss: 0.507, Ça coûte 43.717\n",
      "[22,  5000] loss: 0.523, Ça coûte 44.474\n",
      "[23,  1000] loss: 0.462, Ça coûte 11.888\n",
      "[23,  2000] loss: 0.477, Ça coûte 44.001\n",
      "[23,  3000] loss: 0.482, Ça coûte 45.204\n",
      "[23,  4000] loss: 0.508, Ça coûte 44.791\n",
      "[23,  5000] loss: 0.499, Ça coûte 44.225\n",
      "[24,  1000] loss: 0.446, Ça coûte 11.397\n",
      "[24,  2000] loss: 0.469, Ça coûte 44.302\n",
      "[24,  3000] loss: 0.464, Ça coûte 44.479\n",
      "[24,  4000] loss: 0.461, Ça coûte 45.098\n",
      "[24,  5000] loss: 0.481, Ça coûte 43.782\n",
      "[25,  1000] loss: 0.431, Ça coûte 12.316\n",
      "[25,  2000] loss: 0.432, Ça coûte 45.382\n",
      "[25,  3000] loss: 0.455, Ça coûte 44.177\n",
      "[25,  4000] loss: 0.432, Ça coûte 44.667\n",
      "[25,  5000] loss: 0.451, Ça coûte 44.466\n",
      "[26,  1000] loss: 0.404, Ça coûte 12.556\n",
      "[26,  2000] loss: 0.408, Ça coûte 43.897\n",
      "[26,  3000] loss: 0.431, Ça coûte 41.856\n",
      "[26,  4000] loss: 0.419, Ça coûte 42.279\n",
      "[26,  5000] loss: 0.417, Ça coûte 42.801\n",
      "[27,  1000] loss: 0.375, Ça coûte 11.553\n",
      "[27,  2000] loss: 0.376, Ça coûte 42.737\n",
      "[27,  3000] loss: 0.388, Ça coûte 42.596\n",
      "[27,  4000] loss: 0.413, Ça coûte 41.366\n",
      "[27,  5000] loss: 0.406, Ça coûte 40.355\n",
      "[28,  1000] loss: 0.351, Ça coûte 10.563\n",
      "[28,  2000] loss: 0.374, Ça coûte 38.462\n",
      "[28,  3000] loss: 0.371, Ça coûte 38.738\n",
      "[28,  4000] loss: 0.378, Ça coûte 39.132\n",
      "[28,  5000] loss: 0.382, Ça coûte 39.058\n",
      "[29,  1000] loss: 0.336, Ça coûte 10.031\n",
      "[29,  2000] loss: 0.338, Ça coûte 35.030\n",
      "[29,  3000] loss: 0.359, Ça coûte 34.948\n",
      "[29,  4000] loss: 0.371, Ça coûte 34.541\n",
      "[29,  5000] loss: 0.359, Ça coûte 34.882\n",
      "[30,  1000] loss: 0.315, Ça coûte 9.867\n",
      "[30,  2000] loss: 0.322, Ça coûte 34.678\n",
      "[30,  3000] loss: 0.327, Ça coûte 36.361\n",
      "[30,  4000] loss: 0.339, Ça coûte 35.824\n",
      "[30,  5000] loss: 0.339, Ça coûte 35.064\n",
      "[31,  1000] loss: 0.290, Ça coûte 10.038\n",
      "[31,  2000] loss: 0.301, Ça coûte 34.745\n",
      "[31,  3000] loss: 0.308, Ça coûte 34.926\n",
      "[31,  4000] loss: 0.334, Ça coûte 35.055\n",
      "[31,  5000] loss: 0.316, Ça coûte 34.982\n",
      "[32,  1000] loss: 0.278, Ça coûte 10.449\n",
      "[32,  2000] loss: 0.271, Ça coûte 36.895\n",
      "[32,  3000] loss: 0.295, Ça coûte 37.305\n",
      "[32,  4000] loss: 0.286, Ça coûte 37.716\n",
      "[32,  5000] loss: 0.302, Ça coûte 38.023\n",
      "[33,  1000] loss: 0.251, Ça coûte 10.429\n",
      "[33,  2000] loss: 0.265, Ça coûte 35.778\n",
      "[33,  3000] loss: 0.267, Ça coûte 36.273\n",
      "[33,  4000] loss: 0.262, Ça coûte 36.197\n",
      "[33,  5000] loss: 0.270, Ça coûte 36.180\n",
      "[34,  1000] loss: 0.226, Ça coûte 10.127\n",
      "[34,  2000] loss: 0.237, Ça coûte 35.791\n",
      "[34,  3000] loss: 0.246, Ça coûte 35.927\n",
      "[34,  4000] loss: 0.258, Ça coûte 36.108\n",
      "[34,  5000] loss: 0.271, Ça coûte 36.342\n",
      "[35,  1000] loss: 0.215, Ça coûte 10.307\n",
      "[35,  2000] loss: 0.221, Ça coûte 36.173\n",
      "[35,  3000] loss: 0.213, Ça coûte 36.283\n",
      "[35,  4000] loss: 0.238, Ça coûte 44.071\n",
      "[35,  5000] loss: 0.237, Ça coûte 49.240\n",
      "[36,  1000] loss: 0.189, Ça coûte 17.244\n",
      "[36,  2000] loss: 0.202, Ça coûte 64.553\n",
      "[36,  3000] loss: 0.206, Ça coûte 64.839\n",
      "[36,  4000] loss: 0.215, Ça coûte 65.584\n",
      "[36,  5000] loss: 0.219, Ça coûte 65.222\n",
      "[37,  1000] loss: 0.181, Ça coûte 18.569\n",
      "[37,  2000] loss: 0.191, Ça coûte 62.462\n",
      "[37,  3000] loss: 0.182, Ça coûte 61.630\n",
      "[37,  4000] loss: 0.198, Ça coûte 64.496\n",
      "[37,  5000] loss: 0.194, Ça coûte 66.537\n",
      "[38,  1000] loss: 0.149, Ça coûte 16.066\n",
      "[38,  2000] loss: 0.172, Ça coûte 59.806\n",
      "[38,  3000] loss: 0.171, Ça coûte 60.945\n",
      "[38,  4000] loss: 0.176, Ça coûte 61.376\n",
      "[38,  5000] loss: 0.188, Ça coûte 63.282\n",
      "[39,  1000] loss: 0.144, Ça coûte 16.291\n",
      "[39,  2000] loss: 0.149, Ça coûte 61.948\n",
      "[39,  3000] loss: 0.151, Ça coûte 62.079\n",
      "[39,  4000] loss: 0.157, Ça coûte 62.397\n",
      "[39,  5000] loss: 0.165, Ça coûte 62.914\n",
      "[40,  1000] loss: 0.121, Ça coûte 17.300\n",
      "[40,  2000] loss: 0.132, Ça coûte 62.122\n",
      "[40,  3000] loss: 0.144, Ça coûte 62.151\n",
      "[40,  4000] loss: 0.145, Ça coûte 61.874\n",
      "[40,  5000] loss: 0.143, Ça coûte 58.111\n",
      "[41,  1000] loss: 0.118, Ça coûte 14.726\n",
      "[41,  2000] loss: 0.120, Ça coûte 56.272\n",
      "[41,  3000] loss: 0.116, Ça coûte 53.784\n",
      "[41,  4000] loss: 0.126, Ça coûte 50.467\n",
      "[41,  5000] loss: 0.137, Ça coûte 53.160\n",
      "[42,  1000] loss: 0.098, Ça coûte 15.379\n",
      "[42,  2000] loss: 0.112, Ça coûte 54.392\n",
      "[42,  3000] loss: 0.107, Ça coûte 53.264\n",
      "[42,  4000] loss: 0.118, Ça coûte 53.118\n",
      "[42,  5000] loss: 0.115, Ça coûte 56.118\n",
      "[43,  1000] loss: 0.089, Ça coûte 14.526\n",
      "[43,  2000] loss: 0.083, Ça coûte 53.364\n",
      "[43,  3000] loss: 0.094, Ça coûte 53.847\n",
      "[43,  4000] loss: 0.101, Ça coûte 55.176\n",
      "[43,  5000] loss: 0.102, Ça coûte 54.655\n",
      "[44,  1000] loss: 0.073, Ça coûte 15.533\n",
      "[44,  2000] loss: 0.084, Ça coûte 56.802\n",
      "[44,  3000] loss: 0.089, Ça coûte 55.452\n",
      "[44,  4000] loss: 0.090, Ça coûte 53.817\n",
      "[44,  5000] loss: 0.083, Ça coûte 53.479\n",
      "[45,  1000] loss: 0.062, Ça coûte 14.219\n",
      "[45,  2000] loss: 0.069, Ça coûte 57.201\n",
      "[45,  3000] loss: 0.073, Ça coûte 56.917\n",
      "[45,  4000] loss: 0.068, Ça coûte 57.104\n",
      "[45,  5000] loss: 0.079, Ça coûte 54.820\n",
      "[46,  1000] loss: 0.057, Ça coûte 16.906\n",
      "[46,  2000] loss: 0.056, Ça coûte 73.792\n",
      "[46,  3000] loss: 0.058, Ça coûte 76.194\n",
      "[46,  4000] loss: 0.063, Ça coûte 76.867\n",
      "[46,  5000] loss: 0.061, Ça coûte 78.077\n",
      "[47,  1000] loss: 0.046, Ça coûte 17.961\n",
      "[47,  2000] loss: 0.046, Ça coûte 75.227\n",
      "[47,  3000] loss: 0.048, Ça coûte 76.286\n",
      "[47,  4000] loss: 0.050, Ça coûte 73.436\n",
      "[47,  5000] loss: 0.059, Ça coûte 74.909\n",
      "[48,  1000] loss: 0.041, Ça coûte 21.047\n",
      "[48,  2000] loss: 0.038, Ça coûte 72.093\n",
      "[48,  3000] loss: 0.039, Ça coûte 72.568\n",
      "[48,  4000] loss: 0.039, Ça coûte 76.027\n",
      "[48,  5000] loss: 0.043, Ça coûte 74.320\n",
      "[49,  1000] loss: 0.030, Ça coûte 18.436\n",
      "[49,  2000] loss: 0.031, Ça coûte 73.243\n",
      "[49,  3000] loss: 0.035, Ça coûte 75.141\n",
      "[49,  4000] loss: 0.033, Ça coûte 73.487\n",
      "[49,  5000] loss: 0.036, Ça coûte 74.960\n",
      "[50,  1000] loss: 0.023, Ça coûte 19.122\n",
      "[50,  2000] loss: 0.022, Ça coûte 75.051\n",
      "[50,  3000] loss: 0.026, Ça coûte 72.339\n",
      "[50,  4000] loss: 0.027, Ça coûte 74.058\n",
      "[50,  5000] loss: 0.029, Ça coûte 73.591\n",
      "[51,  1000] loss: 0.021, Ça coûte 19.158\n",
      "[51,  2000] loss: 0.020, Ça coûte 73.941\n",
      "[51,  3000] loss: 0.020, Ça coûte 73.698\n",
      "[51,  4000] loss: 0.023, Ça coûte 74.115\n",
      "[51,  5000] loss: 0.022, Ça coûte 75.808\n",
      "[52,  1000] loss: 0.014, Ça coûte 19.838\n",
      "[52,  2000] loss: 0.014, Ça coûte 73.214\n",
      "[52,  3000] loss: 0.017, Ça coûte 74.150\n",
      "[52,  4000] loss: 0.016, Ça coûte 74.666\n",
      "[52,  5000] loss: 0.015, Ça coûte 74.349\n",
      "[53,  1000] loss: 0.013, Ça coûte 18.281\n",
      "[53,  2000] loss: 0.012, Ça coûte 72.020\n",
      "[53,  3000] loss: 0.013, Ça coûte 76.344\n",
      "[53,  4000] loss: 0.013, Ça coûte 76.494\n",
      "[53,  5000] loss: 0.012, Ça coûte 75.839\n",
      "[54,  1000] loss: 0.010, Ça coûte 19.551\n",
      "[54,  2000] loss: 0.011, Ça coûte 73.272\n",
      "[54,  3000] loss: 0.011, Ça coûte 73.658\n",
      "[54,  4000] loss: 0.010, Ça coûte 76.288\n",
      "[54,  5000] loss: 0.010, Ça coûte 73.190\n",
      "[55,  1000] loss: 0.008, Ça coûte 19.068\n",
      "[55,  2000] loss: 0.008, Ça coûte 75.479\n",
      "[55,  3000] loss: 0.009, Ça coûte 74.846\n",
      "[55,  4000] loss: 0.010, Ça coûte 86.371\n",
      "[55,  5000] loss: 0.009, Ça coûte 90.743\n",
      "[56,  1000] loss: 0.007, Ça coûte 21.431\n",
      "[56,  2000] loss: 0.008, Ça coûte 86.755\n",
      "[56,  3000] loss: 0.007, Ça coûte 89.837\n",
      "[56,  4000] loss: 0.008, Ça coûte 88.278\n",
      "[56,  5000] loss: 0.008, Ça coûte 94.433\n",
      "[57,  1000] loss: 0.006, Ça coûte 26.097\n",
      "[57,  2000] loss: 0.007, Ça coûte 119.660\n",
      "[57,  3000] loss: 0.007, Ça coûte 124.965\n",
      "[57,  4000] loss: 0.006, Ça coûte 115.470\n",
      "[57,  5000] loss: 0.007, Ça coûte 116.004\n",
      "[58,  1000] loss: 0.006, Ça coûte 27.779\n",
      "[58,  2000] loss: 0.006, Ça coûte 119.239\n",
      "[58,  3000] loss: 0.006, Ça coûte 121.070\n",
      "[58,  4000] loss: 0.006, Ça coûte 124.961\n",
      "[58,  5000] loss: 0.006, Ça coûte 125.578\n",
      "[59,  1000] loss: 0.005, Ça coûte 24.889\n",
      "[59,  2000] loss: 0.005, Ça coûte 120.425\n",
      "[59,  3000] loss: 0.005, Ça coûte 123.301\n",
      "[59,  4000] loss: 0.006, Ça coûte 123.982\n",
      "[59,  5000] loss: 0.005, Ça coûte 120.162\n",
      "[60,  1000] loss: 0.005, Ça coûte 26.907\n",
      "[60,  2000] loss: 0.005, Ça coûte 118.275\n",
      "[60,  3000] loss: 0.005, Ça coûte 118.520\n",
      "[60,  4000] loss: 0.005, Ça coûte 114.074\n",
      "[60,  5000] loss: 0.005, Ça coûte 125.357\n",
      "[61,  1000] loss: 0.004, Ça coûte 27.747\n",
      "[61,  2000] loss: 0.004, Ça coûte 118.052\n",
      "[61,  3000] loss: 0.004, Ça coûte 119.968\n",
      "[61,  4000] loss: 0.004, Ça coûte 134.942\n",
      "[61,  5000] loss: 0.004, Ça coûte 165.870\n",
      "[62,  1000] loss: 0.004, Ça coûte 30.949\n",
      "[62,  2000] loss: 0.004, Ça coûte 180.942\n",
      "[62,  3000] loss: 0.004, Ça coûte 161.653\n",
      "[62,  4000] loss: 0.004, Ça coûte 187.261\n",
      "[62,  5000] loss: 0.004, Ça coûte 139.764\n",
      "[63,  1000] loss: 0.004, Ça coûte 37.026\n",
      "[63,  2000] loss: 0.004, Ça coûte 156.063\n",
      "[63,  3000] loss: 0.004, Ça coûte 177.577\n",
      "[63,  4000] loss: 0.004, Ça coûte 150.243\n",
      "[63,  5000] loss: 0.004, Ça coûte 192.061\n",
      "[64,  1000] loss: 0.004, Ça coûte 34.767\n",
      "[64,  2000] loss: 0.004, Ça coûte 149.304\n",
      "[64,  3000] loss: 0.003, Ça coûte 85.121\n",
      "[64,  4000] loss: 0.004, Ça coûte 83.601\n",
      "[64,  5000] loss: 0.004, Ça coûte 58.313\n",
      "[65,  1000] loss: 0.003, Ça coûte 9.915\n",
      "[65,  2000] loss: 0.003, Ça coûte 35.075\n",
      "[65,  3000] loss: 0.003, Ça coûte 35.550\n",
      "[65,  4000] loss: 0.003, Ça coûte 43.387\n",
      "[65,  5000] loss: 0.003, Ça coûte 46.781\n",
      "[66,  1000] loss: 0.003, Ça coûte 10.328\n",
      "[66,  2000] loss: 0.003, Ça coûte 53.327\n",
      "[66,  3000] loss: 0.003, Ça coûte 56.947\n",
      "[66,  4000] loss: 0.003, Ça coûte 56.199\n",
      "[66,  5000] loss: 0.003, Ça coûte 54.191\n",
      "[67,  1000] loss: 0.003, Ça coûte 13.753\n",
      "[67,  2000] loss: 0.003, Ça coûte 57.967\n",
      "[67,  3000] loss: 0.003, Ça coûte 56.816\n",
      "[67,  4000] loss: 0.003, Ça coûte 56.817\n",
      "[67,  5000] loss: 0.003, Ça coûte 55.179\n",
      "[68,  1000] loss: 0.003, Ça coûte 14.246\n",
      "[68,  2000] loss: 0.003, Ça coûte 57.910\n",
      "[68,  3000] loss: 0.003, Ça coûte 56.493\n",
      "[68,  4000] loss: 0.003, Ça coûte 55.801\n",
      "[68,  5000] loss: 0.003, Ça coûte 55.532\n",
      "[69,  1000] loss: 0.002, Ça coûte 14.007\n",
      "[69,  2000] loss: 0.003, Ça coûte 56.069\n",
      "[69,  3000] loss: 0.003, Ça coûte 55.855\n",
      "[69,  4000] loss: 0.003, Ça coûte 54.596\n",
      "[69,  5000] loss: 0.003, Ça coûte 54.145\n",
      "[70,  1000] loss: 0.002, Ça coûte 14.141\n",
      "[70,  2000] loss: 0.002, Ça coûte 56.330\n",
      "[70,  3000] loss: 0.003, Ça coûte 56.133\n",
      "[70,  4000] loss: 0.003, Ça coûte 56.578\n",
      "[70,  5000] loss: 0.003, Ça coûte 56.498\n",
      "[71,  1000] loss: 0.002, Ça coûte 13.919\n",
      "[71,  2000] loss: 0.002, Ça coûte 56.963\n",
      "[71,  3000] loss: 0.002, Ça coûte 56.384\n",
      "[71,  4000] loss: 0.002, Ça coûte 55.171\n",
      "[71,  5000] loss: 0.002, Ça coûte 58.461\n",
      "[72,  1000] loss: 0.002, Ça coûte 12.562\n",
      "[72,  2000] loss: 0.002, Ça coûte 58.202\n",
      "[72,  3000] loss: 0.002, Ça coûte 58.107\n",
      "[72,  4000] loss: 0.002, Ça coûte 57.624\n",
      "[72,  5000] loss: 0.002, Ça coûte 57.421\n",
      "[73,  1000] loss: 0.002, Ça coûte 14.233\n",
      "[73,  2000] loss: 0.002, Ça coûte 56.016\n",
      "[73,  3000] loss: 0.002, Ça coûte 56.602\n",
      "[73,  4000] loss: 0.002, Ça coûte 59.053\n",
      "[73,  5000] loss: 0.002, Ça coûte 58.842\n",
      "[74,  1000] loss: 0.002, Ça coûte 12.958\n",
      "[74,  2000] loss: 0.002, Ça coûte 55.385\n",
      "[74,  3000] loss: 0.002, Ça coûte 57.292\n",
      "[74,  4000] loss: 0.002, Ça coûte 56.651\n",
      "[74,  5000] loss: 0.002, Ça coûte 57.795\n",
      "[75,  1000] loss: 0.002, Ça coûte 14.440\n",
      "[75,  2000] loss: 0.002, Ça coûte 57.882\n",
      "[75,  3000] loss: 0.002, Ça coûte 58.498\n",
      "[75,  4000] loss: 0.002, Ça coûte 57.789\n",
      "[75,  5000] loss: 0.002, Ça coûte 53.289\n",
      "[76,  1000] loss: 0.002, Ça coûte 15.854\n",
      "[76,  2000] loss: 0.002, Ça coûte 56.381\n",
      "[76,  3000] loss: 0.002, Ça coûte 57.298\n",
      "[76,  4000] loss: 0.002, Ça coûte 57.325\n",
      "[76,  5000] loss: 0.002, Ça coûte 56.622\n",
      "[77,  1000] loss: 0.002, Ça coûte 14.553\n",
      "[77,  2000] loss: 0.002, Ça coûte 52.002\n",
      "[77,  3000] loss: 0.002, Ça coûte 52.516\n",
      "[77,  4000] loss: 0.002, Ça coûte 55.557\n",
      "[77,  5000] loss: 0.002, Ça coûte 56.624\n",
      "[78,  1000] loss: 0.002, Ça coûte 13.097\n",
      "[78,  2000] loss: 0.002, Ça coûte 54.443\n",
      "[78,  3000] loss: 0.002, Ça coûte 56.262\n",
      "[78,  4000] loss: 0.002, Ça coûte 56.703\n",
      "[78,  5000] loss: 0.002, Ça coûte 57.424\n",
      "[79,  1000] loss: 0.002, Ça coûte 13.646\n",
      "[79,  2000] loss: 0.002, Ça coûte 57.455\n",
      "[79,  3000] loss: 0.002, Ça coûte 57.137\n",
      "[79,  4000] loss: 0.002, Ça coûte 59.013\n",
      "[79,  5000] loss: 0.002, Ça coûte 58.088\n",
      "[80,  1000] loss: 0.002, Ça coûte 14.951\n",
      "[80,  2000] loss: 0.002, Ça coûte 60.018\n",
      "[80,  3000] loss: 0.002, Ça coûte 59.828\n",
      "[80,  4000] loss: 0.002, Ça coûte 59.155\n",
      "[80,  5000] loss: 0.002, Ça coûte 57.569\n",
      "[81,  1000] loss: 0.002, Ça coûte 15.163\n",
      "[81,  2000] loss: 0.002, Ça coûte 55.700\n",
      "[81,  3000] loss: 0.002, Ça coûte 56.909\n",
      "[81,  4000] loss: 0.002, Ça coûte 55.915\n",
      "[81,  5000] loss: 0.002, Ça coûte 55.821\n",
      "[82,  1000] loss: 0.002, Ça coûte 14.256\n",
      "[82,  2000] loss: 0.002, Ça coûte 54.699\n",
      "[82,  3000] loss: 0.002, Ça coûte 56.121\n",
      "[82,  4000] loss: 0.002, Ça coûte 56.576\n",
      "[82,  5000] loss: 0.002, Ça coûte 56.635\n",
      "[83,  1000] loss: 0.002, Ça coûte 13.466\n",
      "[83,  2000] loss: 0.002, Ça coûte 55.628\n",
      "[83,  3000] loss: 0.001, Ça coûte 57.339\n",
      "[83,  4000] loss: 0.002, Ça coûte 56.239\n",
      "[83,  5000] loss: 0.002, Ça coûte 56.343\n",
      "[84,  1000] loss: 0.001, Ça coûte 14.969\n",
      "[84,  2000] loss: 0.001, Ça coûte 52.706\n",
      "[84,  3000] loss: 0.002, Ça coûte 55.249\n",
      "[84,  4000] loss: 0.002, Ça coûte 57.653\n",
      "[84,  5000] loss: 0.001, Ça coûte 58.505\n",
      "[85,  1000] loss: 0.001, Ça coûte 14.210\n",
      "[85,  2000] loss: 0.001, Ça coûte 58.351\n",
      "[85,  3000] loss: 0.002, Ça coûte 58.214\n",
      "[85,  4000] loss: 0.001, Ça coûte 175.563\n",
      "[85,  5000] loss: 0.001, Ça coûte 285.758\n",
      "[86,  1000] loss: 0.001, Ça coûte 39.253\n",
      "[86,  2000] loss: 0.001, Ça coûte 287.201\n",
      "[86,  3000] loss: 0.001, Ça coûte 285.461\n",
      "[86,  4000] loss: 0.001, Ça coûte 283.323\n",
      "[86,  5000] loss: 0.001, Ça coûte 286.744\n",
      "[87,  1000] loss: 0.001, Ça coûte 42.394\n",
      "[87,  2000] loss: 0.001, Ça coûte 291.342\n",
      "[87,  3000] loss: 0.001, Ça coûte 269.219\n",
      "[87,  4000] loss: 0.001, Ça coûte 289.233\n",
      "[87,  5000] loss: 0.001, Ça coûte 288.604\n",
      "[88,  1000] loss: 0.001, Ça coûte 39.375\n",
      "[88,  2000] loss: 0.001, Ça coûte 285.931\n",
      "[88,  3000] loss: 0.001, Ça coûte 289.063\n",
      "[88,  4000] loss: 0.001, Ça coûte 293.459\n",
      "[88,  5000] loss: 0.001, Ça coûte 289.071\n",
      "[89,  1000] loss: 0.001, Ça coûte 47.111\n",
      "[89,  2000] loss: 0.001, Ça coûte 301.493\n",
      "[89,  3000] loss: 0.001, Ça coûte 298.151\n",
      "[89,  4000] loss: 0.001, Ça coûte 287.321\n",
      "[89,  5000] loss: 0.001, Ça coûte 289.737\n",
      "[90,  1000] loss: 0.001, Ça coûte 42.193\n",
      "[90,  2000] loss: 0.001, Ça coûte 294.829\n",
      "[90,  3000] loss: 0.001, Ça coûte 294.346\n",
      "[90,  4000] loss: 0.001, Ça coûte 285.959\n",
      "[90,  5000] loss: 0.001, Ça coûte 293.976\n",
      "[91,  1000] loss: 0.001, Ça coûte 39.664\n",
      "[91,  2000] loss: 0.001, Ça coûte 287.835\n",
      "[91,  3000] loss: 0.001, Ça coûte 287.966\n",
      "[91,  4000] loss: 0.001, Ça coûte 280.786\n",
      "[91,  5000] loss: 0.001, Ça coûte 286.517\n",
      "[92,  1000] loss: 0.001, Ça coûte 39.302\n",
      "[92,  2000] loss: 0.001, Ça coûte 260.758\n",
      "[92,  3000] loss: 0.001, Ça coûte 286.403\n",
      "[92,  4000] loss: 0.001, Ça coûte 285.284\n",
      "[92,  5000] loss: 0.001, Ça coûte 276.530\n",
      "[93,  1000] loss: 0.001, Ça coûte 39.395\n",
      "[93,  2000] loss: 0.001, Ça coûte 245.051\n",
      "[93,  3000] loss: 0.001, Ça coûte 214.835\n",
      "[93,  4000] loss: 0.001, Ça coûte 195.675\n",
      "[93,  5000] loss: 0.001, Ça coûte 190.878\n",
      "[94,  1000] loss: 0.001, Ça coûte 38.013\n",
      "[94,  2000] loss: 0.001, Ça coûte 203.137\n",
      "[94,  3000] loss: 0.001, Ça coûte 195.081\n",
      "[94,  4000] loss: 0.001, Ça coûte 192.489\n",
      "[94,  5000] loss: 0.001, Ça coûte 195.474\n",
      "[95,  1000] loss: 0.001, Ça coûte 40.136\n",
      "[95,  2000] loss: 0.001, Ça coûte 174.731\n",
      "[95,  3000] loss: 0.001, Ça coûte 175.277\n",
      "[95,  4000] loss: 0.001, Ça coûte 193.569\n",
      "[95,  5000] loss: 0.001, Ça coûte 193.041\n",
      "[96,  1000] loss: 0.001, Ça coûte 40.498\n",
      "[96,  2000] loss: 0.001, Ça coûte 174.573\n",
      "[96,  3000] loss: 0.001, Ça coûte 176.091\n",
      "[96,  4000] loss: 0.001, Ça coûte 189.588\n",
      "[96,  5000] loss: 0.001, Ça coûte 195.045\n",
      "[97,  1000] loss: 0.001, Ça coûte 40.563\n",
      "[97,  2000] loss: 0.001, Ça coûte 174.362\n",
      "[97,  3000] loss: 0.001, Ça coûte 176.284\n",
      "[97,  4000] loss: 0.001, Ça coûte 193.386\n",
      "[97,  5000] loss: 0.001, Ça coûte 193.758\n",
      "[98,  1000] loss: 0.001, Ça coûte 40.764\n",
      "[98,  2000] loss: 0.001, Ça coûte 172.945\n",
      "[98,  3000] loss: 0.001, Ça coûte 178.962\n",
      "[98,  4000] loss: 0.001, Ça coûte 192.554\n",
      "[98,  5000] loss: 0.001, Ça coûte 190.936\n",
      "[99,  1000] loss: 0.001, Ça coûte 40.640\n",
      "[99,  2000] loss: 0.001, Ça coûte 173.337\n",
      "[99,  3000] loss: 0.001, Ça coûte 178.889\n",
      "[99,  4000] loss: 0.001, Ça coûte 192.799\n",
      "[99,  5000] loss: 0.001, Ça coûte 192.802\n",
      "[100,  1000] loss: 0.001, Ça coûte 40.647\n",
      "[100,  2000] loss: 0.001, Ça coûte 174.695\n",
      "[100,  3000] loss: 0.001, Ça coûte 176.316\n",
      "[100,  4000] loss: 0.001, Ça coûte 191.136\n",
      "[100,  5000] loss: 0.001, Ça coûte 193.242\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (600,) and (500,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 209\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(beta_1, beta_2), eps=1e-08, weight_decay=0, amsgrad=False)\u001b[39;00m\n\u001b[0;32m    208\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39mbeta_1, dampening\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 209\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExp1-training-loss-beta1=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbeta_1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;bs=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExp1-test-accuracy-beta1=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbeta_1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;bs=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExp1-train-accuracy-beta1=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbeta_1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;bs=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    211\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cifar_net.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 110\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, noe, filepath_trainloss, filepath_testacc, filepath_trainacc, initial_lr, batchsize, device)\u001b[0m\n\u001b[0;32m    108\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    109\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_loss_vec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(filepath_trainloss\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m#plt.subplot(122)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProjectBreez\\gpu_env\\lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   3796\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[0;32m   3797\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   3798\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3799\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3800\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProjectBreez\\gpu_env\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\ProjectBreez\\gpu_env\\lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProjectBreez\\gpu_env\\lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (600,) and (500,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHJCAYAAABpOFaGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAs8UlEQVR4nO3dC5zN1f7/8c+YMTPmMC5hBk2uuZVLGYT8pDAdHrqXkJkk0j1KSC6lKCdSUUqkmyjRqTgqRKkpuXWUW0KjDoMwIzTDzPf/+Kzz3/vMZg8zmn1dr+fjsZu9v/v73Xvt+cZ+W+uz1jfCcRxHAAAALFQq0A0AAAAIFIIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAQBiIiIop1q1WrVom3QV9TXzvYXqukuX6HAMJDVKAbAOCvS0tLO2XbypUr5eeff5ZmzZpJ8+bNPZ6rXLmyH1sHAMGLIASEgVmzZp2y7dZbbzVB6JprrpExY8b4vA1Lly6V48ePB91rAcDpEIQAlIi6desG5WsBwOlQIwRY2HukNS7aS7R161a5+eabJSEhQUqVKiUffPCB2Wfbtm3m+TZt2khiYqJER0fLueeeK6mpqeaYotb17Ny502y77LLL5NixYzJs2DCpWbOmxMTESL169eTpp58Wx3F8+lpqxYoVcvnll0u5cuWkYsWK0rVrV1m9erXH78JXdu3aJXfccYe7rVWrVpXrrrtOvvvuO6/7//DDD3LLLbdInTp1JDY2VqpUqWKGNh944AHZvXu3x75ff/216fFzvbaeq1atWpnfzR9//OGzzwSEE3qEAEtt2bJFWrZsKeecc4507NhRDh48KKVLlzbPvfrqqzJhwgS58MILzT76Jbtx40Z588035Z///Kd8+eWX0rRp0yK/V25urnTp0sW8hgaZI0eOmHCiX9iHDx+WJ554wmevNX/+fLnpppskLy9PLrnkEhOyNmzYIJdeeqn07dtXfEnfRwPY/v37pUGDBiYAZWRkyIIFC+Sjjz6S2bNny4033ujef82aNaZdf/75p/n9Xn311XL06FHZvn27PPfccyb0VKtWzeyrx+tjDX8aftq2bSuHDh2Sn376yYTCgQMHStmyZX36+YCw4AAIS2lpado94owePdpj+2uvvWa26+2ee+5xTpw4ccqx6enpzvbt20/ZPnPmTHNcx44dT3muZs2a5rmCduzY4X6vDh06OFlZWe7nvvvuOycyMtKJi4tzDh8+7JPX0n0qVapkjnn77bc9Xm/kyJHu1zv5d3Q6rmPOJD8/32nSpInZ9+GHHzaPXebNm+eUKlXKKVu2rPOf//zHvT01NdXs/8wzz5zyeps2bfLY9//+7//MvvpaJ1u1apWTnZ1d5M8E2IyhMcBSOuSiPQeRkZGnPKc9J7Vr1z5lu/agtGvXTpYvXy5ZWVlFfi8ddnv55ZclPj7evS05OVn+/ve/mx4PHabyxWu9++67cuDAAbniiiukV69eHq8zatQoM6TkK/o70h6h8847z/RSFRzqu/76601vjg5fzZw5071937595menTp1Oeb2GDRu6e4POtK/24ukwIIAzIwgBltIv0Li4uEKf1y/pd955R4YOHSr9+/c3s9D0pnUq2jGiM9KKSgOHDg2drH79+ubnybUvJfVaX331lflZcPjJJSoqygQSX9HhQ6XDcq4hx4L69OnjsZ9q0aKF+Xn33XebIHXixIlCX9+1r76O1hvl5+eX+GcAbECNEGAp7akozLJly0wRtavXwRutxykqLbT2xtVrkZOT45PXcoWipKSkYv8O/qr//Oc/5mdhi1e6tv/222/ubUOGDDHrP2kI0rotrfHRgvVu3bqZEFq+fHn3vuPGjTM9TlorpDctAtf6oquuusoUW2uhNYAzo0cIsFRhX5TaE6S9GFrgq8NHWpSsBcna46A9QT179jT7FTZDq7DhrJJSkq8VSN5Wp9bhPg2h2kv08MMPS+PGjc1jnTGmvWBaCO2i4U6HAT/55BO59957zWMNRNp7p4XWv//+u58/ERCawuNvFAAlRr+E9UtUh40ee+wxadSokRlCc31x6wymUOGqqdEp7N4Utr0kVK9e3fz85ZdfvD6vywGoGjVqeGzX37P27Gj91rfffmt6ljR8ZmZmyogRI04Z3tMZdM8//7x8//335jV1lppr5hiAMyMIAfCg0+gLG4LS9YXWrl0roUILu9X7779/ynM6nV6n1vtK+/btzc/33nvPvNfJ3nrrLY/9CqPrDrnWOdI1hs5UP6U1XUXZF8B/EYQAeC061pBQsEZI16jp169fSF36QoukK1WqJJ999pnMmTPH4zmdybVjxw6fvbeucdSkSRPTS6NDjAWHEnUdIf39ag3Qbbfd5t4+bdo0r21atGjRKbVOzz77rOzZs6dI+wIoHMXSADzoVPTOnTub8KChSL/QlRbw6sVadZE/XVQxFGhx8fTp003Nkw4v6RCSa0FFXSF7wIAB8sorr5iVs4tLlxgozO23325ub7/9til61sJmDT+6QrQuqKiz2XRYa8aMGR5T4jUI3XnnnaY2SIckdZ/NmzebYS+t6dJA5aLDlg899JC5qO75559vgpbup59Lw58+B+DM6BECcAoNOlqPomsN/etf/zIrHusssm+++UYqVKggoURXc16yZIkJdP/+979l4cKFpn5Ha6Fcs8Z0de3i0vqdwm6//vqr2Ud7hHQoUQuYtQh93rx5ZkVvXUNIw5AGtILGjh1reoi0TkgvPKvFz3o5EQ1V69evdw/1qRdeeMGcE107Sc/R4sWLTXAaPHiw+ZwajgCcWYSuqliE/QAg7Fx55ZVm1pUGvNatWwe6OQACgB4hAGFN1+nRGVcF6VIAWmOjIUiH//RaXQDsRI0QgLCmQ2C6wOBFF11kZlXpgos6o0qLmHVZAL3ArLc1fQDYgaExAGFN19QZP368CUTaM6RXdk9MTDQ1Q3rFei1MBmCvgA6NffHFF9K9e3dTuKj/Ivvggw/OeIzOXLn44oslJiZG6tWrJ7NmzfJLWwGEJi0a1gubaiDKzs6W3NxcM3PrjTfeIAQBCGwQ0mX7dern1KlTi7S/rq+h19zR6ag6g0KXndfZFDrODwAAELJDY9ojpOts6LTSwuiKqTr1teCKqTp9VBd606mjAAAAYVssnZ6eLp06dfLYlpKSYnqGCqOFkQWvRq2zRQ4cOGDWDaFAEgCA0KD9NocPHzblNCV58eWQCkK6nHxCQoLHNn2s4/666FiZMmVOOUaLJHUFVgAAEPp27drl9VqIVgShszF8+HCz0qpLVlaWWU1Wf5Hx8fEBbRsAACga7fTQa+iVK1dOSlJIBSGd8nrywmj6WAONt94gpbPL9HYyPYYgBABAaCnpspaQWlm6TZs25vo7BemFIXU7AABASAUhvQihToPXm2t6vN7XNT5cw1qpqanu/QcOHCjbt2+Xhx9+2FyR+cUXX5R3331XBg0aFLDPAAAAQldAg9Dq1avNsvd6U1rLo/dHjRplHu/evdsdilTt2rXN9HntBdL1hyZOnGiWx9eZYwAAACG7jpA/i63Kly9viqapEQIAwO7v75CqEQIAAChJBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAawU8CE2dOlVq1aolsbGx0rp1a1m1atVp9588ebI0aNBAypQpI0lJSTJo0CD5888//dZeAAAQPgIahObOnSuDBw+W0aNHy9q1a6VZs2aSkpIie/fu9br/7NmzZdiwYWb/TZs2yYwZM8xrPPLII35vOwAACH0BDUKTJk2S/v37S9++faVx48Yybdo0iYuLk5kzZ3rd/+uvv5Z27dpJr169TC9Sly5dpGfPnmfsRQIAAAiqIJSbmytr1qyRTp06/a8xpUqZx+np6V6Padu2rTnGFXy2b98uixYtkq5duxb6Pjk5OZKdne1xAwAAUFGB+jXs379f8vLyJCEhwWO7Pt68ebPXY7QnSI+79NJLxXEcOXHihAwcOPC0Q2Pjx4+Xxx57rMTbDwAAQl/Ai6WLY/ny5TJu3Dh58cUXTU3R/PnzZeHChTJ27NhCjxk+fLhkZWW5b7t27fJrmwEAQPAKWI9Q5cqVJTIyUjIzMz226+PExESvx4wcOVL69Okjt99+u3ncpEkTOXLkiAwYMEBGjBhhhtZOFhMTY24AAABB0yMUHR0tLVq0kKVLl7q35efnm8dt2rTxeszRo0dPCTsappQOlQEAAIREj5DSqfNpaWmSnJwsrVq1MmsEaQ+PziJTqampUqNGDVPno7p3725mml100UVmzaFt27aZXiLd7gpEAAAAIRGEevToIfv27ZNRo0bJnj17pHnz5rJ48WJ3AXVGRoZHD9Cjjz4qERER5udvv/0mVapUMSHoySefDOCnAAAAoSrCsWxMSafPly9f3hROx8fHB7o5AAAggN/fITVrDAAAoCQRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACs9ZeDUHZ2tnzwwQeyadOmszp+6tSpUqtWLYmNjZXWrVvLqlWrTrv/oUOH5O6775Zq1apJTEyM1K9fXxYtWnSWrQcAADYrdhC66aabZMqUKeb+sWPHJDk52Wxr2rSpvP/++8V6rblz58rgwYNl9OjRsnbtWmnWrJmkpKTI3r17ve6fm5srnTt3lp07d8q8efNky5YtMn36dKlRo0ZxPwYAAEDxg9AXX3wh7du3N/cXLFggjuOYXprnn39ennjiiWK91qRJk6R///7St29fady4sUybNk3i4uJk5syZXvfX7QcOHDA9UO3atTM9SR06dDABCgAAwOdBKCsrSypVqmTuL168WK6//noTXrp16yY//fRTkV9He3fWrFkjnTp1+l9jSpUyj9PT070e8+GHH0qbNm3M0FhCQoJceOGFMm7cOMnLyyv0fXJycszwXcEbAADAWQWhpKQkE1SOHDliglCXLl3M9oMHD5o6n6Lav3+/CTAaaArSx3v27PF6zPbt282QmB6ndUEjR46UiRMnnrYnavz48VK+fHn3TdsPAABwVkHogQcekN69e8u5554r1atXl8suu8w9ZNakSROf/lbz8/OlatWq8sorr0iLFi2kR48eMmLECDOkVpjhw4ebXizXbdeuXT5tIwAACB1RxT3grrvuklatWplAoYXLOpyl6tSpU6waocqVK0tkZKRkZmZ6bNfHiYmJXo/RmWKlS5c2x7k0atTI9CDpUFt0dPQpx+jMMr0BAACUyPR5nSl27bXXStmyZc0w1fr166Vt27amgLmoNLRor87SpUs9enz0sdYBeaOvv23bNrOfy9atW01A8haCAAAASnxobMaMGea+hiCdtXXxxReb2pvly5cX67V06rxOf3/99dfNOkR33nmnqT3SWWQqNTXVDG256PM6a+z+++83AWjhwoWmWFqLpwEAAHw+NKbFyrfccou5/9FHH8mOHTtk8+bN8uabb5p6na+++qrIr6U1Pvv27ZNRo0aZ4a3mzZubAmxXAXVGRoZ76E1p2Prkk09k0KBBZt0iXT9IQ9HQoUOL+zEAAAAkwtGFgIpBZ4bp8JQWSw8YMMBMnZ88ebIJRLqeT7BPT9f26ewxLZyOj48PdHMAAEAAv7+LPTSmvTUbN240w2Lae6MF0+ro0aMeRcwAAABhNzSm9Tt6SQ0tUI6IiHAviPjtt99Kw4YNfdFGAACA4AhCY8aMMSs66/T5G2+80T01XXuDhg0b5os2AgAABEeNUKijRggAgNCTHSw1QmrFihXSvXt3qVevnrldddVV8uWXX5ZYowAAAPyh2EHorbfeMnVBOlvsvvvuM7cyZcrIFVdcIbNnz/ZNKwEAAIJhaEwvaaHT5nUtn4ImTZpkFkfUhRGDGUNjAACEnuxgGRrTK8DrsNjJdHhM1xICAAAIFcUOQrq6c8Hrg7ksWbLEPAcAABC20+cffPBBUxfkutCq0stqzJo1S5577jlftBEAACA4gpBe+DQxMVEmTpwo7777rrtuaO7cuXL11Vf7oo0AAAA+wTpCAAAg6AVNsTQAAIBVQ2MVK1Y01xUrigMHDvzVNgEAAARPEJo8ebLvWwIAABCMQSgtLc33LQEAAPAzaoQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFir2JfYuPbaa72uKaTbYmNjpV69etKrVy9p0KBBSbURAAAgOHqEdHnrZcuWydq1a0340du6devMthMnTphrjjVr1sxciBUAACCseoT0gqva4zNlyhQpVeq/OSo/P1/uv/9+KVeunMyZM0cGDhwoQ4cOlZUrV/qizQAAAIG56GqVKlVMb0/9+vU9tm/dulXatm0r+/fvlw0bNkj79u3l0KFDEmy46CoAAKEnO1guuqrDX5s3bz5lu27Ly8sz97VWqKjXJgMAAAiZobE+ffpIv3795JFHHpGWLVuabd99952MGzdOUlNTzeMVK1bIBRdcUPKtBQAACGQQevbZZyUhIUEmTJggmZmZZps+HjRokKkLUl26dJErr7yyJNsJAAAQ+Bqhk8frVCjV2lAjBABA6Mn20fd3sXuECiJIAACAUFbsYmkdDtM6oerVq0tUVJRERkZ63AAAAEJFsXuEbr31VsnIyJCRI0dKtWrVmB0GAADsCUK6SOKXX34pzZs3902LAAAAgnVoLCkpSf5CfTUAAEDoBqHJkyfLsGHDZOfOnb5pEQAAQLAOjfXo0UOOHj0qdevWlbi4OCldurTH8wcOHCjJ9gEAAARPENIeIQAAACuDUFpamm9aAgAAEIxBSFdzdC2e6FpNujAssggAAMIqCFWsWFF2794tVatWlQoVKnhdO0hnkul21xXoAQAAwiIILVu2TCpVqmTuf/75575uEwAAQPBfdDUUcdFVAABCT3YwXXT10KFDsmrVKtm7d6/k5+d7PJeamlpSbQMAAPCpYgehjz76SHr37i1//PGHSWQF64X0PkEIAACE7crSDz74oNx2220mCGnP0MGDB903FlMEAABhHYR+++03ue+++8yq0gAAAFYFoZSUFFm9erVvWgMAABDMNULdunWTIUOGyMaNG6VJkyanXGvsqquuKsn2AQAABM/0+VKlCu9ECoUFFZk+DwBA6MkOlunzJ0+XBwAAsKZGCAAAIFwUqUfo+eeflwEDBkhsbKy5fzo6owwAACBsaoRq165tZoqdc8455n6hLxYRIdu3b5dgRo0QAAChJzuQNUI7duzweh8AACCUUSMEAACsdVYXXf3111/lww8/lIyMDMnNzfV4btKkSSXVNgAAgOAKQkuXLjWLJtapU0c2b94sF154oezcuVO01Ojiiy/2TSsBAACCYWhs+PDh8tBDD8mGDRvMLLL3339fdu3aJR06dJAbb7zRF20EAAAIjiC0adMmSU1NNfejoqLk2LFjUrZsWXn88cfl6aef9kUbAQAAgiMI/e1vf3PXBVWrVk1+/vln93P79+8v2dYBAAAEU43QJZdcIitXrpRGjRpJ165d5cEHHzTDZPPnzzfPAQAAhG0Q0llhf/zxh7n/2GOPmftz586V888/nxljAAAgfIOQXllep843bdrUPUw2bdo0X7UNAAAgeGqEIiMjpUuXLnLw4MESbcTUqVOlVq1aZhZa69atZdWqVUU6bs6cOeayHtdcc02JtgcAANih2MXSum5QSV5PTIfVBg8eLKNHj5a1a9dKs2bNJCUlRfbu3Xva43TtIp3G3759+xJrCwAAsEuxg9ATTzxhAsjHH38su3fvNhdBK3grLq0r6t+/v/Tt21caN25shtri4uJk5syZpx2i6927t6lR0oUdAQAA/FIsrTPFlK4urcNSLrqytD7WkFJUOg1/zZo1ZpFGl1KlSkmnTp0kPT290ON0zaKqVatKv3795Msvvzzte+Tk5Jiby9mENQAAEJ6KHYQ+//zzEntzXXdIg1NCQoLHdn2sl+/wRqfuz5gxQ9avX1+k9xg/frzpOQIAAPjLQah27dqSlJTk0Rvk6hHSS2340uHDh6VPnz4yffp0qVy5cpGO0d4mrUEq2COk7QcAADirIKS1QTo0VdCBAwfMc8UZGtMwozPRMjMzPbbr48TExFP211WstUi6e/fu7m35+fn//SBRUbJlyxapW7euxzExMTHmBgAA8JeLpV21QCfThRV1+ntxREdHS4sWLcwV7QsGG33cpk2bU/Zv2LChWcVah8VcN61V6tixo7lPTw8AAPBJj5BreElD0MiRI83MLhftBfr222+lefPmxXpz1+umpaVJcnKytGrVSiZPnixHjhwxs8iUXuC1Ro0aptZHg5ZO3y+oQoUK5ufJ2wEAAEosCK1bt87dI6S9Mtqb46L3df0fnVZfXD169JB9+/bJqFGjZM+ePSZMLV682F1AnZGRYWaSAQAAlLQIR5NNMWhPzXPPPSfx8fESirRYunz58pKVlRWynwEAANtk++j7u9jF0q+99lqJvTkAAEAgMeYEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYK2gCEJTp06VWrVqSWxsrLRu3VpWrVpV6L7Tp0+X9u3bS8WKFc2tU6dOp90fAAAgaIPQ3LlzZfDgwTJ69GhZu3atNGvWTFJSUmTv3r1e91++fLn07NlTPv/8c0lPT5ekpCTp0qWL/Pbbb35vOwAACG0RjuM4gWyA9gC1bNlSpkyZYh7n5+ebcHPvvffKsGHDznh8Xl6e6RnS41NTU8+4f3Z2tpQvX16ysrIkPj6+RD4DAADwLV99fwe0Ryg3N1fWrFljhrfcDSpVyjzW3p6iOHr0qBw/flwqVark9fmcnBzzyyt4AwAACHgQ2r9/v+nRSUhI8Niuj/fs2VOk1xg6dKhUr17dI0wVNH78eJMgXTftbQIAAAiKGqG/4qmnnpI5c+bIggULTKG1N8OHDzfdaK7brl27/N5OAAAQnKIC+eaVK1eWyMhIyczM9NiujxMTE0977DPPPGOC0JIlS6Rp06aF7hcTE2NuAAAAQdUjFB0dLS1atJClS5e6t2mxtD5u06ZNocdNmDBBxo4dK4sXL5bk5GQ/tRYAAISbgPYIKZ06n5aWZgJNq1atZPLkyXLkyBHp27eveV5ngtWoUcPU+qinn35aRo0aJbNnzzZrD7lqicqWLWtuAAAAIROEevToIfv27TPhRkNN8+bNTU+Pq4A6IyPDzCRzeemll8xssxtuuMHjdXQdojFjxvi9/QAAIHQFfB0hf2MdIQAAQk92OK4jBAAAEEgEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWCoogNHXqVKlVq5bExsZK69atZdWqVafd/7333pOGDRua/Zs0aSKLFi3yW1sBAED4CHgQmjt3rgwePFhGjx4ta9eulWbNmklKSors3bvX6/5ff/219OzZU/r16yfr1q2Ta665xtx++OEHv7cdAACEtgjHcZxANkB7gFq2bClTpkwxj/Pz8yUpKUnuvfdeGTZs2Cn79+jRQ44cOSIff/yxe9sll1wizZs3l2nTpp3x/bKzs6V8+fKSlZUl8fHxJfxpAACAL/jq+zugPUK5ubmyZs0a6dSp0/8aVKqUeZyenu71GN1ecH+lPUiF7Q8AAFCYKAmg/fv3S15eniQkJHhs18ebN2/2esyePXu87q/bvcnJyTE3F02SrmQJAABCg+t7u6QHsgIahPxh/Pjx8thjj52yXYffAABAaPn999/NEFlYBKHKlStLZGSkZGZmemzXx4mJiV6P0e3F2X/48OGmGNvl0KFDUrNmTcnIyCjRXyTOLt1rIN21axf1WkGA8xE8OBfBg3MRPHRE57zzzpNKlSqV6OsGNAhFR0dLixYtZOnSpWbml6tYWh/fc889Xo9p06aNef6BBx5wb/vss8/Mdm9iYmLM7WQagvifOjjoeeBcBA/OR/DgXAQPzkXw0FrisBoa096atLQ0SU5OllatWsnkyZPNrLC+ffua51NTU6VGjRpmiEvdf//90qFDB5k4caJ069ZN5syZI6tXr5ZXXnklwJ8EAACEmoAHIZ0Ov2/fPhk1apQpeNZp8IsXL3YXROsQVsH017ZtW5k9e7Y8+uij8sgjj8j5558vH3zwgVx44YUB/BQAACAUBTwIKR0GK2wobPny5adsu/HGG83tbOgwmS7e6G24DP7FuQgunI/gwbkIHpyL8D8XAV9QEQAAwNpLbAAAAAQKQQgAAFiLIAQAAKxFEAIAANYKyyA0depUqVWrlsTGxpqr269ateq0+7/33nvSsGFDs3+TJk1k0aJFfmtruCvOuZg+fbq0b99eKlasaG56cd0znTv49s+Gi67XFRER4V74FP4/F7oq/t133y3VqlUzs2bq16/P31UBOhe63l2DBg2kTJkyZtXpQYMGyZ9//um39oarL774Qrp37y7Vq1c3f9/o0jhnojPLL774YvNnol69ejJr1qziv7ETZubMmeNER0c7M2fOdH788Uenf//+ToUKFZzMzEyv+3/11VdOZGSkM2HCBGfjxo3Oo48+6pQuXdrZsGGD39tu+7no1auXM3XqVGfdunXOpk2bnFtvvdUpX7688+uvv/q97eGouOfDZceOHU6NGjWc9u3bO1dffbXf2hvOinsucnJynOTkZKdr167OypUrzTlZvny5s379er+33fZz8fbbbzsxMTHmp56HTz75xKlWrZozaNAgv7c93CxatMgZMWKEM3/+fJ3N7ixYsOC0+2/fvt2Ji4tzBg8ebL6/X3jhBfN9vnjx4mK9b9gFoVatWjl33323+3FeXp5TvXp1Z/z48V73v+mmm5xu3bp5bGvdurVzxx13+Lyt4a645+JkJ06ccMqVK+e8/vrrPmylPc7mfOg5aNu2rfPqq686aWlpBKEAnYuXXnrJqVOnjpObm+vHVtqhuOdC97388ss9tukXcbt27XzeVptIEYLQww8/7FxwwQUe23r06OGkpKQU673CamgsNzdX1qxZY4ZUXHRVan2cnp7u9RjdXnB/lZKSUuj+8N25ONnRo0fl+PHjJX6BPRud7fl4/PHHpWrVqtKvXz8/tTT8nc25+PDDD831FHVoTFfd15X0x40bJ3l5eX5sefg5m3OhVzfQY1zDZ9u3bzdDlF27dvVbu1Gy399BsbJ0Sdm/f7/5i8F1eQ4Xfbx582avx+hlPbztr9vh33NxsqFDh5qx4pP/R4d/zsfKlStlxowZsn79ej+10g5ncy70y3bZsmXSu3dv86W7bds2ueuuu8w/FHSlXfjvXPTq1cscd+mll+qIipw4cUIGDhxoLvkE/yrs+zs7O1uOHTtmariKIqx6hBA+nnrqKVOgu2DBAlPACP86fPiw9OnTxxSwV65cOdDNsV5+fr7pmdOLS7do0cJco3HEiBEybdq0QDfNOlqcq71xL774oqxdu1bmz58vCxculLFjxwa6aThLYdUjpH9hR0ZGSmZmpsd2fZyYmOj1GN1enP3hu3Ph8swzz5ggtGTJEmnatKmPW2qH4p6Pn3/+WXbu3GlmcBT8MlZRUVGyZcsWqVu3rh9aHn7O5s+GzhQrXbq0Oc6lUaNG5l/EOrwTHR3t83aHo7M5FyNHjjT/SLj99tvNY51pfOTIERkwYIAJpwUvEg7fKuz7Oz4+vsi9QSqszpj+ZaD/Wlq6dKnHX976WMfXvdHtBfdXn332WaH7w3fnQk2YMMH8y2rx4sWSnJzsp9aGv+KeD11OYsOGDWZYzHW76qqrpGPHjua+ThmG//5stGvXzgyHucKo2rp1qwlIhCD/ngutXTw57LgCKpfu9K8S+/52wnAqpE5tnDVrlplON2DAADMVcs+ePeb5Pn36OMOGDfOYPh8VFeU888wzZsr26NGjmT4foHPx1FNPmWms8+bNc3bv3u2+HT58OICfwt7zcTJmjQXuXGRkZJgZlPfcc4+zZcsW5+OPP3aqVq3qPPHEEwH8FHaeC/2O0HPxzjvvmOnbn376qVO3bl0zAxl/jf5dr8un6E3jyaRJk8z9X375xTyv50HPx8nT54cMGWK+v3X5FabP/3+6lsB5551nvlR1auQ333zjfq5Dhw7mL/SC3n33Xad+/fpmf52Kt3DhwgC0OjwV51zUrFnT/M9/8k3/4kFg/mwURBAK7Ln4+uuvzdIe+qWtU+mffPJJs7wB/Hsujh8/7owZM8aEn9jYWCcpKcm56667nIMHDwao9eHj888/9/od4Pr96089Hycf07x5c3Pu9M/Fa6+9Vuz3jdD/lGxnFQAAQGgIqxohAACA4iAIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEwMoLZ0ZERMihQ4cC3RQAAUYQAgAA1iIIAQAAaxGEAPidXuF7/PjxUrt2bSlTpow0a9ZM5s2b5zFstXDhQmnatKnExsbKJZdcIj/88IPHa7z//vtywQUXSExMjNSqVUsmTpzo8XxOTo4MHTpUkpKSzD716tWTGTNmeOyzZs0aSU5Olri4OGnbtq1s2bLF/dz3338vHTt2lHLlykl8fLy5Svnq1at9+nsB4H8EIQB+pyHojTfekGnTpsmPP/4ogwYNkltuuUVWrFjh3mfIkCEm3Hz33XdSpUoV6d69uxw/ftwdYG666Sa5+eabZcOGDTJmzBgZOXKkzJo1y318amqqvPPOO/L888/Lpk2b5OWXX5ayZct6tGPEiBHmPTTgREVFyW233eZ+rnfv3nLuueea99f3GzZsmJQuXdovvx8AflRSV40FgKL4888/nbi4OHM19YL69evn9OzZ030F6jlz5rif+/33350yZco4c+fONY979erldO7c2eP4IUOGOI0bNzb3t2zZYl7js88+89oG13ssWbLEvW3hwoVm27Fjx8zjcuXKObNmzSrBTw4gGNEjBMCvtm3bJkePHpXOnTubHhrXTXuIfv75Z/d+bdq0cd+vVKmSNGjQwPTsKP3Zrl07j9fVxz/99JPk5eXJ+vXrJTIyUjp06HDatujQm0u1atXMz71795qfgwcPlttvv106deokTz31lEfbAIQPghAAv/rjjz/MT60B0sDium3cuNFdJ/RXad1RURQc6tK6JFf9ktLhNh2269atmyxbtkwaN24sCxYsKJH2AQgeBCEAfqWBQouXMzIyTAFzwZsWNrt888037vsHDx6UrVu3SqNGjcxj/fnVV195vK4+rl+/vukJatKkiQk0BWuOzoa+ntYvffrpp3LdddfJa6+99pdeD0DwiQp0AwDYRWdhPfTQQyZgaFi59NJLJSsrywQZnZ1Vs2ZNs9/jjz8u55xzjiQkJJii5sqVK8s111xjnnvwwQelZcuWMnbsWOnRo4ekp6fLlClT5MUXXzTP6yyytLQ0U/ysxdI6K+2XX34xw15aZH0mx44dM8XaN9xwg5nZ9uuvv5qi6euvv97Hvx0AfhfoIiUA9snPz3cmT57sNGjQwCldurRTpUoVJyUlxVmxYoW7kPmjjz5yLrjgAic6Otpp1aqV8/3333u8xrx580xxtB5/3nnnOf/4xz88ntei50GDBjnVqlUzr1GvXj1n5syZ5jnXexw8eNC9/7p168y2HTt2ODk5Oc7NN9/sJCUlmWOrV6/u3HPPPe5CagDhI0L/4//4BQDe6TpCun6PDodVqFAh0M0BEOaoEQIAANYiCAEAAGsxNAYAAKxFjxAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAEFv9P8nX8PX8yPfEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Batch size: 8\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_some_pictures(j ):\n",
    "    # get some random training images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # show images\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    # print labels\n",
    "    print(' '.join('%5s' % classes[labels[j]] for j in range(j)))\n",
    "\n",
    "\n",
    "def random_test():\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # print images\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                                  for j in range(4)))\n",
    "\n",
    "def train(net, noe, filepath_trainloss, filepath_testacc, filepath_trainacc, initial_lr,  batchsize, device):\n",
    "    doc = open(filepath_trainloss+'.txt', \"w\")\n",
    "    doc2 = open(filepath_testacc+'.txt', \"w\")\n",
    "    doc3 = open(filepath_trainacc+'.txt', \"w\")\n",
    "    check_interval=1000\n",
    "    batch_number = int(6000*8/(batchsize*check_interval))\n",
    "    #print(batch_number)\n",
    "    training_loss_vec = [] #np.zeros(noe*check_interval)\n",
    "    train_acc_vec = [] #np.zeros(noe*check_interval)\n",
    "    test_acc_vec = [] #np.zeros(noe*check_interval)\n",
    "    for epoch in range(noe):  # loop over the dataset multiple times\n",
    "        time_begin  = time.time()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % check_interval == (check_interval-1):    # print every 2000 mini-batches\n",
    "                time_end = time.time()\n",
    "                time_elapsed = time_end - time_begin\n",
    "                time_begin = time.time()\n",
    "                print('[%d, %5d] loss: %.3f, Ça coûte %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / check_interval, time_elapsed))\n",
    "                training_loss_vec.append(running_loss/check_interval)\n",
    "                train_acc = train_accuracy(net)\n",
    "                train_acc_vec.append(train_acc)\n",
    "                test_acc = test_accuracy(net)\n",
    "                test_acc_vec.append(test_acc)\n",
    "                print(running_loss / check_interval, file=doc)\n",
    "                print(test_acc, file=doc2)\n",
    "                print(train_acc, file=doc3)\n",
    "                running_loss = 0.0\n",
    "        if epoch % 1 == 0:\n",
    "            for p in optimizer.param_groups:\n",
    "                p['lr'] = initial_lr/np.sqrt(1+epoch)\n",
    "    doc.close()\n",
    "    doc2.close()\n",
    "\n",
    "    xvar=np.arange(noe*batch_number)/batch_number\n",
    "    #plt.subplot(122)\n",
    "    plt.figure(1)\n",
    "    plt.title(\"Training Loss\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('training loss')\n",
    "    plt.plot(xvar, np.array(training_loss_vec))\n",
    "    plt.savefig(filepath_trainloss+'.png')\n",
    "\n",
    "    #plt.subplot(122)\n",
    "    plt.figure(2)\n",
    "    plt.title(\"Test Accuracy\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('test accuracy(%)')\n",
    "    plt.plot(xvar, np.array(test_acc_vec), label=\"Test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(filepath_testacc + '.png')\n",
    "    \n",
    "    #plt.subplot(122)\n",
    "    plt.figure(2)\n",
    "    plt.title(\"Training and Test Accuracy\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('training accuracy(%)')\n",
    "    plt.plot(xvar, np.array(train_acc_vec), label=\"Training accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(filepath_trainacc + '.png')\n",
    "\n",
    "def train_accuracy(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "def test_accuracy(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "\n",
    "batchsize=8\n",
    "# data sets\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "#gotta take one of the classes from the cifar10 dataset and reduce it\n",
    "trainarray = np.array(trainset.targets)\n",
    "\n",
    "#separate the frogs\n",
    "frog_class = np.where(np.array(trainarray) == 6)[0][:2000]\n",
    "\n",
    "#remove frogs from the original train set\n",
    "non_frog_class = np.where(np.array(trainarray) != 6)[0]\n",
    "\n",
    "#now combine them again\n",
    "reducedset = Subset(trainset, np.concatenate((frog_class, non_frog_class)))\n",
    "\n",
    "#now printing the total size to make suer we removed some frogs\n",
    "print(len(reducedset))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(reducedset, batch_size=batchsize,\n",
    "                                          shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize,\n",
    "                                         shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "beta_1 = 0\n",
    "beta_2 = 1\n",
    "\n",
    "# resnet\n",
    "net = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
    "net.eval()\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(beta_1, beta_2), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=beta_1, dampening=0, weight_decay=0, nesterov=False)\n",
    "train(net, 100, \"Exp1-training-loss-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), \"Exp1-test-accuracy-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), \"Exp1-train-accuracy-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), 0.001, batchsize, device)\n",
    "print('Finished Training')\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0aeb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Batch size: 16\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_some_pictures(j ):\n",
    "    # get some random training images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # show images\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    # print labels\n",
    "    print(' '.join('%5s' % classes[labels[j]] for j in range(j)))\n",
    "\n",
    "\n",
    "def random_test():\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # print images\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                                  for j in range(4)))\n",
    "\n",
    "def train(net, noe, filepath_trainloss, filepath_testacc, filepath_trainacc, initial_lr,  batchsize, device):\n",
    "    doc = open(filepath_trainloss+'.txt', \"w\")\n",
    "    doc2 = open(filepath_testacc+'.txt', \"w\")\n",
    "    doc3 = open(filepath_trainacc+'.txt', \"w\")\n",
    "    check_interval=1000\n",
    "    batch_number = int(6000*8/(batchsize*check_interval))\n",
    "    #print(batch_number)\n",
    "    training_loss_vec = [] #np.zeros(noe*check_interval)\n",
    "    train_acc_vec = [] #np.zeros(noe*check_interval)\n",
    "    test_acc_vec = [] #np.zeros(noe*check_interval)\n",
    "    for epoch in range(noe):  # loop over the dataset multiple times\n",
    "        time_begin  = time.time()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % check_interval == (check_interval-1):    # print every 2000 mini-batches\n",
    "                time_end = time.time()\n",
    "                time_elapsed = time_end - time_begin\n",
    "                time_begin = time.time()\n",
    "                print('[%d, %5d] loss: %.3f, Ça coûte %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / check_interval, time_elapsed))\n",
    "                training_loss_vec.append(running_loss/check_interval)\n",
    "                train_acc = train_accuracy(net)\n",
    "                train_acc_vec.append(train_acc)\n",
    "                test_acc = test_accuracy(net)\n",
    "                test_acc_vec.append(test_acc)\n",
    "                print(running_loss / check_interval, file=doc)\n",
    "                print(test_acc, file=doc2)\n",
    "                print(train_acc, file=doc3)\n",
    "                running_loss = 0.0\n",
    "        if epoch % 1 == 0:\n",
    "            for p in optimizer.param_groups:\n",
    "                p['lr'] = initial_lr/np.sqrt(1+epoch)\n",
    "    doc.close()\n",
    "    doc2.close()\n",
    "\n",
    "    xvar=np.arange(noe*batch_number)/batch_number\n",
    "    #plt.subplot(122)\n",
    "    plt.figure(1)\n",
    "    plt.title(\"Training Loss\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('training loss')\n",
    "    plt.plot(xvar, np.array(training_loss_vec))\n",
    "    plt.savefig(filepath_trainloss+'.png')\n",
    "\n",
    "    #plt.subplot(122)\n",
    "    plt.figure(2)\n",
    "    plt.title(\"Test Accuracy\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('test accuracy(%)')\n",
    "    plt.plot(xvar, np.array(test_acc_vec), label=\"Test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(filepath_testacc + '.png')\n",
    "    \n",
    "    #plt.subplot(122)\n",
    "    plt.figure(2)\n",
    "    plt.title(\"Training and Test Accuracy\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('training accuracy(%)')\n",
    "    plt.plot(xvar, np.array(train_acc_vec), label=\"Training accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(filepath_trainacc + '.png')\n",
    "\n",
    "def train_accuracy(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "def test_accuracy(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "\n",
    "batchsize=16\n",
    "# data sets\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "#gotta take one of the classes from the cifar10 dataset and reduce it\n",
    "trainarray = np.array(trainset.targets)\n",
    "\n",
    "#separate the frogs\n",
    "frog_class = np.where(np.array(trainarray) == 6)[0][:2000]\n",
    "\n",
    "#remove frogs from the original train set\n",
    "non_frog_class = np.where(np.array(trainarray) != 6)[0]\n",
    "\n",
    "#now combine them again\n",
    "reducedset = Subset(trainset, np.concatenate((frog_class, non_frog_class)))\n",
    "\n",
    "#now printing the total size to make suer we removed some frogs\n",
    "print(len(reducedset))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(reducedset, batch_size=batchsize,\n",
    "                                          shuffle=True, num_workers=1, pin_memory=True,)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize,\n",
    "                                         shuffle=False, num_workers=1, pin_memory=True,)\n",
    "\n",
    "beta_1 = 0\n",
    "beta_2 = 1\n",
    "\n",
    "# resnet\n",
    "net = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
    "net.eval()\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(beta_1, beta_2), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=beta_1, dampening=0, weight_decay=0, nesterov=False)\n",
    "train(net, 100, \"Exp1-training-loss-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), \"Exp1-test-accuracy-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), \"Exp1-train-accuracy-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), 0.001, batchsize, device)\n",
    "print('Finished Training')\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Batch size: 32\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_some_pictures(j ):\n",
    "    # get some random training images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # show images\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    # print labels\n",
    "    print(' '.join('%5s' % classes[labels[j]] for j in range(j)))\n",
    "\n",
    "\n",
    "def random_test():\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # print images\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                                  for j in range(4)))\n",
    "\n",
    "def train(net, noe, filepath_trainloss, filepath_testacc, filepath_trainacc, initial_lr,  batchsize, device):\n",
    "    doc = open(filepath_trainloss+'.txt', \"w\")\n",
    "    doc2 = open(filepath_testacc+'.txt', \"w\")\n",
    "    doc3 = open(filepath_trainacc+'.txt', \"w\")\n",
    "    check_interval=1000\n",
    "    batch_number = int(6000*8/(batchsize*check_interval))\n",
    "    #print(batch_number)\n",
    "    training_loss_vec = [] #np.zeros(noe*check_interval)\n",
    "    train_acc_vec = [] #np.zeros(noe*check_interval)\n",
    "    test_acc_vec = [] #np.zeros(noe*check_interval)\n",
    "    for epoch in range(noe):  # loop over the dataset multiple times\n",
    "        time_begin  = time.time()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % check_interval == (check_interval-1):    # print every 2000 mini-batches\n",
    "                time_end = time.time()\n",
    "                time_elapsed = time_end - time_begin\n",
    "                time_begin = time.time()\n",
    "                print('[%d, %5d] loss: %.3f, Ça coûte %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / check_interval, time_elapsed))\n",
    "                training_loss_vec.append(running_loss/check_interval)\n",
    "                train_acc = train_accuracy(net)\n",
    "                train_acc_vec.append(train_acc)\n",
    "                test_acc = test_accuracy(net)\n",
    "                test_acc_vec.append(test_acc)\n",
    "                print(running_loss / check_interval, file=doc)\n",
    "                print(test_acc, file=doc2)\n",
    "                print(train_acc, file=doc3)\n",
    "                running_loss = 0.0\n",
    "        if epoch % 1 == 0:\n",
    "            for p in optimizer.param_groups:\n",
    "                p['lr'] = initial_lr/np.sqrt(1+epoch)\n",
    "    doc.close()\n",
    "    doc2.close()\n",
    "\n",
    "    xvar=np.arange(noe*batch_number)/batch_number\n",
    "    #plt.subplot(122)\n",
    "    plt.figure(1)\n",
    "    plt.title(\"Training Loss\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('training loss')\n",
    "    plt.plot(xvar, np.array(training_loss_vec))\n",
    "    plt.savefig(filepath_trainloss+'.png')\n",
    "\n",
    "    #plt.subplot(122)\n",
    "    plt.figure(2)\n",
    "    plt.title(\"Test Accuracy\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('test accuracy(%)')\n",
    "    plt.plot(xvar, np.array(test_acc_vec), label=\"Test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(filepath_testacc + '.png')\n",
    "    \n",
    "    #plt.subplot(122)\n",
    "    plt.figure(2)\n",
    "    plt.title(\"Training and Test Accuracy\", fontsize=15)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('training accuracy(%)')\n",
    "    plt.plot(xvar, np.array(train_acc_vec), label=\"Training accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(filepath_trainacc + '.png')\n",
    "\n",
    "def train_accuracy(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "def test_accuracy(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "\n",
    "batchsize=32\n",
    "# data sets\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "#gotta take one of the classes from the cifar10 dataset and reduce it\n",
    "trainarray = np.array(trainset.targets)\n",
    "\n",
    "#separate the frogs\n",
    "frog_class = np.where(np.array(trainarray) == 6)[0][:2000]\n",
    "\n",
    "#remove frogs from the original train set\n",
    "non_frog_class = np.where(np.array(trainarray) != 6)[0]\n",
    "\n",
    "#now combine them again\n",
    "reducedset = Subset(trainset, np.concatenate((frog_class, non_frog_class)))\n",
    "\n",
    "#now printing the total size to make suer we removed some frogs\n",
    "print(len(reducedset))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(reducedset, batch_size=batchsize,\n",
    "                                          shuffle=True, num_workers=1, pin_memory=True,)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize,\n",
    "                                         shuffle=False, num_workers=1, pin_memory=True,)\n",
    "\n",
    "beta_1 = 0\n",
    "beta_2 = 1\n",
    "\n",
    "# resnet\n",
    "net = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
    "net.eval()\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(beta_1, beta_2), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=beta_1, dampening=0, weight_decay=0, nesterov=False)\n",
    "train(net, 100, \"Exp1-training-loss-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), \"Exp1-test-accuracy-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), \"Exp1-train-accuracy-beta1=\"+str(beta_1)+\";bs=\"+str(batchsize), 0.001, batchsize, device)\n",
    "print('Finished Training')\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
