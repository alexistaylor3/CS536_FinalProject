{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Batch Size: 32\n",
        "Beta 2: 0.99\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def show_some_pictures(j ):\n",
        "    # get some random training images\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "    # show images\n",
        "    imshow(torchvision.utils.make_grid(images))\n",
        "    # print labels\n",
        "    print(' '.join('%5s' % classes[labels[j]] for j in range(j)))\n",
        "\n",
        "\n",
        "def random_test():\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "    # print images\n",
        "    imshow(torchvision.utils.make_grid(images))\n",
        "    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "    net = Net()\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    outputs = net(images)\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                                  for j in range(4)))\n",
        "\n",
        "def train(net, noe, filepath_trainloss, filepath_testacc, filepath_trainacc, initial_lr,  batchsize, device):\n",
        "    net.train()\n",
        "    doc = open(filepath_trainloss+'.txt', \"w\")\n",
        "    doc2 = open(filepath_testacc+'.txt', \"w\")\n",
        "    doc3 = open(filepath_trainacc+'.txt', \"w\")\n",
        "    check_interval=1000\n",
        "    batch_number = int(6000*8/(batchsize*check_interval))\n",
        "    #print(batch_number)\n",
        "    training_loss_vec = [] #np.zeros(noe*check_interval)\n",
        "    train_acc_vec = [] #np.zeros(noe*check_interval)\n",
        "    test_acc_vec = [] #np.zeros(noe*check_interval)\n",
        "    for epoch in range(noe):  # loop over the dataset multiple times\n",
        "        time_begin  = time.time()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % check_interval == (check_interval-1):    # print every 2000 mini-batches\n",
        "                time_end = time.time()\n",
        "                time_elapsed = time_end - time_begin\n",
        "                time_begin = time.time()\n",
        "                print('[%d, %5d] loss: %.3f, Ça coûte %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / check_interval, time_elapsed))\n",
        "                training_loss_vec.append(running_loss/check_interval)\n",
        "                train_acc = train_accuracy(net)\n",
        "                train_acc_vec.append(train_acc)\n",
        "                test_acc = test_accuracy(net)\n",
        "                test_acc_vec.append(test_acc)\n",
        "                print(running_loss / check_interval, file=doc)\n",
        "                print(test_acc, file=doc2)\n",
        "                print(train_acc, file=doc3)\n",
        "                running_loss = 0.0\n",
        "        if epoch % 5 == 0:\n",
        "            for p in optimizer.param_groups:\n",
        "                p['lr'] *= 0.9\n",
        "    doc.close()\n",
        "    doc2.close()\n",
        "\n",
        "    xvar=np.arange(noe*batch_number)/batch_number\n",
        "    #plt.subplot(122)\n",
        "    plt.figure(1)\n",
        "    plt.title(\"Training Loss\", fontsize=15)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('training loss')\n",
        "    plt.plot(xvar, np.array(training_loss_vec))\n",
        "    plt.savefig(filepath_trainloss+'.png')\n",
        "\n",
        "    #plt.subplot(122)\n",
        "    plt.figure(2)\n",
        "    plt.title(\"Test Accuracy\", fontsize=15)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('test accuracy(%)')\n",
        "    plt.plot(xvar, np.array(test_acc_vec), label=\"Test accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(filepath_testacc + '.png')\n",
        "    \n",
        "    #plt.subplot(122)\n",
        "    plt.figure(2)\n",
        "    plt.title(\"Training and Test Accuracy\", fontsize=15)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('training accuracy(%)')\n",
        "    plt.plot(xvar, np.array(train_acc_vec), label=\"Training accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(filepath_trainacc + '.png')\n",
        "\n",
        "def train_accuracy(net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def test_accuracy(net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "batchsize=32\n",
        "# data sets\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize,\n",
        "                                          shuffle=True, num_workers=1)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize,\n",
        "                                         shuffle=False, num_workers=1)\n",
        "\n",
        "beta_1 = 0.0\n",
        "beta_2 = 0.99\n",
        "\n",
        "# resnet\n",
        "net = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
        "net.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "net.maxpool = nn.Identity()\n",
        "# net.eval()\n",
        "\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(beta_1, beta_2), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "train(net, 100, \"Exp1-training-loss-beta1=\"+str(beta_1)+\";beta2=\"+str(beta_2)+\";bs=\"+str(batchsize), \"Exp1-test-accuracy-beta1=\"+str(beta_1)+\";beta2=\"+str(beta_2)+\";bs=\"+str(batchsize), \"Exp1-train-accuracy-beta1=\"+str(beta_1)+\";beta2=\"+str(beta_2)+\";bs=\"+str(batchsize), 0.001, batchsize, device)\n",
        "print('Finishegit d Training')\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Batch Size: 512\n",
        "Beta 2: 0.99\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def show_some_pictures(j ):\n",
        "    # get some random training images\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "    # show images\n",
        "    imshow(torchvision.utils.make_grid(images))\n",
        "    # print labels\n",
        "    print(' '.join('%5s' % classes[labels[j]] for j in range(j)))\n",
        "\n",
        "\n",
        "def random_test():\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "    # print images\n",
        "    imshow(torchvision.utils.make_grid(images))\n",
        "    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "    net = Net()\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    outputs = net(images)\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                                  for j in range(4)))\n",
        "\n",
        "def train(net, noe, filepath_trainloss, filepath_testacc, filepath_trainacc, initial_lr,  batchsize, device):\n",
        "    net.train()\n",
        "    doc = open(filepath_trainloss+'.txt', \"w\")\n",
        "    doc2 = open(filepath_testacc+'.txt', \"w\")\n",
        "    doc3 = open(filepath_trainacc+'.txt', \"w\")\n",
        "    check_interval=1000\n",
        "    batch_number = int(6000*8/(batchsize*check_interval))\n",
        "    #print(batch_number)\n",
        "    training_loss_vec = [] #np.zeros(noe*check_interval)\n",
        "    train_acc_vec = [] #np.zeros(noe*check_interval)\n",
        "    test_acc_vec = [] #np.zeros(noe*check_interval)\n",
        "    for epoch in range(noe):  # loop over the dataset multiple times\n",
        "        time_begin  = time.time()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % check_interval == (check_interval-1):    # print every 2000 mini-batches\n",
        "                time_end = time.time()\n",
        "                time_elapsed = time_end - time_begin\n",
        "                time_begin = time.time()\n",
        "                print('[%d, %5d] loss: %.3f, Ça coûte %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / check_interval, time_elapsed))\n",
        "                training_loss_vec.append(running_loss/check_interval)\n",
        "                train_acc = train_accuracy(net)\n",
        "                train_acc_vec.append(train_acc)\n",
        "                test_acc = test_accuracy(net)\n",
        "                test_acc_vec.append(test_acc)\n",
        "                print(running_loss / check_interval, file=doc)\n",
        "                print(test_acc, file=doc2)\n",
        "                print(train_acc, file=doc3)\n",
        "                running_loss = 0.0\n",
        "        if epoch % 5 == 0:\n",
        "            for p in optimizer.param_groups:\n",
        "                p['lr'] *= 0.9\n",
        "    doc.close()\n",
        "    doc2.close()\n",
        "\n",
        "    xvar=np.arange(noe*batch_number)/batch_number\n",
        "    #plt.subplot(122)\n",
        "    plt.figure(1)\n",
        "    plt.title(\"Training Loss\", fontsize=15)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('training loss')\n",
        "    plt.plot(xvar, np.array(training_loss_vec))\n",
        "    plt.savefig(filepath_trainloss+'.png')\n",
        "\n",
        "    #plt.subplot(122)\n",
        "    plt.figure(2)\n",
        "    plt.title(\"Test Accuracy\", fontsize=15)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('test accuracy(%)')\n",
        "    plt.plot(xvar, np.array(test_acc_vec), label=\"Test accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(filepath_testacc + '.png')\n",
        "    \n",
        "    #plt.subplot(122)\n",
        "    plt.figure(2)\n",
        "    plt.title(\"Training and Test Accuracy\", fontsize=15)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('training accuracy(%)')\n",
        "    plt.plot(xvar, np.array(train_acc_vec), label=\"Training accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(filepath_trainacc + '.png')\n",
        "\n",
        "def train_accuracy(net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def test_accuracy(net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "batchsize=512\n",
        "# data sets\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize,\n",
        "                                          shuffle=True, num_workers=1)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize,\n",
        "                                         shuffle=False, num_workers=1)\n",
        "\n",
        "beta_1 = 0.0\n",
        "beta_2 = 0.99\n",
        "\n",
        "# resnet\n",
        "net = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
        "# net.eval()\n",
        "\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(beta_1, beta_2), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "train(net, 100, \"Exp1-training-loss-beta1=\"+str(beta_1)+\";beta2=\"+str(beta_2)+\";bs=\"+str(batchsize), \"Exp1-test-accuracy-beta1=\"+str(beta_1)+\";beta2=\"+str(beta_2)+\";bs=\"+str(batchsize), \"Exp1-train-accuracy-beta1=\"+str(beta_1)+\";beta2=\"+str(beta_2)+\";bs=\"+str(batchsize), 0.001, batchsize, device)\n",
        "print('Finishegit d Training')\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
